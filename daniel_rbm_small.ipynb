{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine for Small Dataset (ml-latest-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.neural_network import BernoulliRBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Dataset:\n",
    "Before running this notebook, please download **ml-latest-small.zip** from https://grouplens.org/datasets/movielens/latest/ and extract the folder to the same directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie ids in movies.csv are not sequential.\n",
    "\n",
    "There are 9742 movies listed but the highest movie id is 193609."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of movies: 9742\n"
     ]
    }
   ],
   "source": [
    "movies = {}\n",
    "\n",
    "with open('ml-latest-small/movies.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i != 0:\n",
    "            m_id = int(row[0])\n",
    "            movies[m_id] = row[1:]\n",
    "\n",
    "# retrieves movie index based on movie id\n",
    "m_index_lookup = {}\n",
    "            \n",
    "ordered = sorted(movies.items())\n",
    "for i, (k, v) in enumerate(ordered):\n",
    "    m_index_lookup[k] = i\n",
    "    \n",
    "n_movies = len(movies)\n",
    "print(\"# of movies:\", n_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users shape: (610, 9742)\n"
     ]
    }
   ],
   "source": [
    "# 3.0 is threshold for favorable rating\n",
    "def convert_rating(rating: str):\n",
    "    r = float(rating)\n",
    "    if (r < 3.0):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "users = []\n",
    "curr_id = 0\n",
    "\n",
    "with open('ml-latest-small/ratings.csv', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i != 0:\n",
    "            u_id = int(row[0])\n",
    "            if u_id != curr_id:\n",
    "                users.append(np.full(n_movies, -1))\n",
    "                curr_id = u_id\n",
    "            ratings = users[-1]\n",
    "            m_id = int(row[1])\n",
    "            m_index = m_index_lookup[m_id]\n",
    "            m_rating = convert_rating(row[2])\n",
    "            ratings[m_index] = m_rating\n",
    "\n",
    "# convert to 2d numpy array\n",
    "users = np.array(users)\n",
    "\n",
    "print(\"users shape:\", users.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "610 users with 9742 movie ratings each. Each rating is either 1(like), 0(dislike), or -1(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average # ratings per user: 165.30491803278687\n"
     ]
    }
   ],
   "source": [
    "def average_ratings_per_user(users):\n",
    "    total = 0\n",
    "    for user in users:\n",
    "        # element-wise addition\n",
    "        arr = user + 1\n",
    "        # numpy has an advanced feature called Boolean Array Indexing\n",
    "        # sets each element in arr that is greater than 0 to 1\n",
    "        arr[arr > 0] = 1\n",
    "        total += np.sum(arr)\n",
    "    print(\"average # ratings per user:\", total / len(users))\n",
    "    \n",
    "average_ratings_per_user(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How will the RBM fare with such a sparse matrix of ratings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Data (Optional)\n",
    "**Try fitting the RBM without processing the data first**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_rows(arr, col=0):\n",
    "    return arr[np.argsort(arr[:, col])]\n",
    "\n",
    "# selects k rows with the highest # of ratings\n",
    "# TODO: update movie index\n",
    "def filter(arr, k):\n",
    "    arr2 = np.copy(arr)\n",
    "    arr2 += 1\n",
    "    arr2[arr2 > 0] = 1\n",
    "    sums = np.empty((arr2.shape[0], 2))\n",
    "    for i, row in enumerate(arr2):\n",
    "        sums[i, 0] = np.sum(row)\n",
    "        sums[i, 1] = i\n",
    "    sums = sort_rows(sums)\n",
    "    sums = sums[::-1] # reverse array\n",
    "#     for i in range(k):\n",
    "#         print(sums[i])\n",
    "    indices = sums[:k, 1].T.astype(int)\n",
    "    indices = np.sort(indices)\n",
    "#     print(indices)\n",
    "    top_k = arr[indices]\n",
    "#     print(top_k.shape)\n",
    "    return top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Filter:\n",
      "(610, 9742)\n",
      "average # ratings per user: 165.30491803278687\n",
      "\n",
      "After filtering top 1000 rated movies:\n",
      "(610, 1000)\n",
      "average # ratings per user: 100.41967213114754\n",
      "\n",
      "After filtering top 300 user raters:\n",
      "(300, 1000)\n",
      "average # ratings per user: 173.26\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Filter:\")\n",
    "print(users.shape)\n",
    "average_ratings_per_user(users)\n",
    "\n",
    "print()\n",
    "users2 = filter(users.T, 1000).T\n",
    "print(\"After filtering top 1000 rated movies:\")\n",
    "print(users2.shape)\n",
    "average_ratings_per_user(users2)\n",
    "\n",
    "print()\n",
    "users3 = filter(users2, 300)\n",
    "print(\"After filtering top 300 user raters:\")\n",
    "print(users3.shape)\n",
    "average_ratings_per_user(users3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data into training / testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPUTE_MODE = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some users are tough reviewers; others are more forgiving\n",
    "# This function will randomly assign 1s and 0s based on their current review probabilities\n",
    "def impute_missing(X, mode=\"default\"):\n",
    "    if mode == \"default\":\n",
    "        for i, row in enumerate(X):\n",
    "            # calculate how likely the user will give\n",
    "            # a posive review\n",
    "            neg = np.sum(row == 0)\n",
    "            pos = np.sum(row == 1)\n",
    "            p = pos / (pos + neg)\n",
    "\n",
    "            missing = row == -1\n",
    "            imputed = np.random.rand(np.sum(missing))\n",
    "            imputed = imputed < p\n",
    "            row[missing] = imputed.astype(int)\n",
    "            \n",
    "    elif mode == \"random\":\n",
    "        for row in X:\n",
    "            missing = row == -1\n",
    "            imputed = np.random.rand(np.sum(missing))\n",
    "            imputed = imputed < 0.5\n",
    "            row[missing] = imputed.astype(int)\n",
    "            \n",
    "    elif mode == \"zero\":\n",
    "        X[X < 0] = 0\n",
    "        \n",
    "def split_data(data, mode=\"default\"):\n",
    "    pi = np.random.permutation(data.shape[0])\n",
    "    split = int(data.shape[0] * 0.8)\n",
    "\n",
    "    Xtr_missing = data[pi[:split], :]\n",
    "    Xte_missing = data[pi[split:], :]\n",
    "\n",
    "    Xtr, Xte = np.copy(Xtr_missing), np.copy(Xte_missing)\n",
    "\n",
    "    # Imputing Missing Values\n",
    "    impute_missing(Xtr, mode)\n",
    "    impute_missing(Xte, mode)\n",
    "    \n",
    "    return Xtr, Xte, Xtr_missing, Xte_missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape: (240, 1000)\n",
      "testing shape: (60, 1000)\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xte, Xtr_missing, Xte_missing = split_data(users3, mode=IMPUTE_MODE)\n",
    "\n",
    "print(\"training shape:\", Xtr.shape)\n",
    "print(\"testing shape:\", Xte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data to RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliRBM(batch_size=10, learning_rate=0.1, n_components=16, n_iter=100,\n",
       "             random_state=0, verbose=0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbm = BernoulliRBM(\n",
    "    n_components = 16,\n",
    "    learning_rate = 0.1,\n",
    "    batch_size = 10,\n",
    "    n_iter = 100,\n",
    "    verbose = 0,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "# this might take a minute or two\n",
    "rbm.fit(Xtr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood?\n",
    "\n",
    "This score fluctuates wildly based every time I run the rbm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-326.5997556457192\n"
     ]
    }
   ],
   "source": [
    "print(rbm.score_samples(Xte).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_missing(rbm, Xobs, n_iters=10, mode=\"default\"):\n",
    "    Xhat = np.copy(Xobs)\n",
    "    # impute missing values\n",
    "    impute_missing(Xhat, mode)\n",
    "    # print(\"preprocess done\")\n",
    "    for i in range(n_iters):\n",
    "        Xhat = rbm.gibbs(Xhat).astype(int)\n",
    "        Xhat[Xobs >= 0] = Xobs[Xobs >= 0]\n",
    "    return Xhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, Xte_missing has a handful (~165) of ratings per user, with the rest being missing. My plan is to conceal some of these known ratings and see whether or not the RBM can recover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each user has rated at least 20 movies\n",
    "\n",
    "# create mask marking all known ratings\n",
    "mask = Xte_missing + 1\n",
    "mask[mask > 0] = 1\n",
    "\n",
    "Xte_concealed = np.copy(Xte_missing)\n",
    "for user, mask_row in zip(Xte_concealed, mask):\n",
    "    indices = mask_row.nonzero()[0]\n",
    "    n_ratings = indices.shape[0]\n",
    "    indices = np.random.permutation(indices)\n",
    "    split = int(n_ratings * 0.3)\n",
    "    # set 30% of known ratings to missing\n",
    "    user[indices[:split]] = -1\n",
    "    # turn off bits for the latter 70% part of the mask\n",
    "    mask_row[indices[split:]] = 0\n",
    "    \n",
    "# Xte_concealed will now be same as Xte_missing \n",
    "#   but with 30% of known ratings concealed.\n",
    "# mask will mark the locations of all ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xte_predict = sample_missing(rbm, Xte_concealed, n_iters = 100, mode = IMPUTE_MODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 719\n",
      "Total Checks: 2794\n",
      "Error Rate: 0.2573371510379384\n",
      "RMSE: 0.5072840930267165\n"
     ]
    }
   ],
   "source": [
    "num_checks = np.sum(mask)\n",
    "arr1 = Xte_missing * mask\n",
    "arr2 = Xte_predict * mask\n",
    "result = arr1 ^ arr2\n",
    "num_err = np.sum(result)\n",
    "\n",
    "print(\"Errors:\", num_err)\n",
    "print(\"Total Checks:\", num_checks)\n",
    "print(\"Error Rate:\", num_err / num_checks)\n",
    "print(\"RMSE:\", np.sqrt(num_err/num_checks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this sucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives:  2091\n",
      "True Negatives:  63\n",
      "False Positives: 417\n",
      "False Negatives: 223\n",
      "\n",
      "Accuracy:  0.7709377236936292\n",
      "Precision: 0.833732057416268\n",
      "Recall:    0.9036300777873811\n"
     ]
    }
   ],
   "source": [
    "# When recommending movies to a user, it is better to not recommend good movies than to recommend bad movies\n",
    "# False Positive: Recommend a movie when the user doesn't actually like the movie\n",
    "# False Negative: Not recommend a movie when the user does like the movie\n",
    "\n",
    "# In this case, I think False Positives are worse than False Negatives as it would cause the user to ignore\n",
    "# the recommendation system\n",
    "\n",
    "arr1_b = arr1.astype(bool)\n",
    "arr2_b = arr2.astype(bool)\n",
    "\n",
    "# false positive:\n",
    "fp_arr = np.invert(arr1_b) & arr2_b\n",
    "# false negative:\n",
    "fn_arr = arr1_b & np.invert(arr2_b)\n",
    "# true positive:\n",
    "tp_arr = arr1_b & arr2_b\n",
    "\n",
    "fp = np.sum(fp_arr)\n",
    "fn = np.sum(fn_arr)\n",
    "tp = np.sum(tp_arr)\n",
    "# since we're checking only a handful of indices\n",
    "# true negative has to be calculated from the 3 other values\n",
    "tn = num_checks - (fp + fn + tp)\n",
    "\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "\n",
    "print()\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
